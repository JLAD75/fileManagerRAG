# Optimisation du systÃ¨me RAG

## ğŸ¯ Objectif
AmÃ©liorer la prÃ©cision et la pertinence des rÃ©ponses de l'IA en exploitant pleinement les capacitÃ©s des modÃ¨les GPT modernes (GPT-4o, GPT-5).

## âœ… Optimisations implÃ©mentÃ©es

### 1. **Contexte dynamique adaptÃ© au modÃ¨le**
**Fichier**: `backend/src/services/chatService.ts`

```typescript
// Calcul dynamique basÃ© sur les capacitÃ©s du modÃ¨le
- GPT-5: 400,000 tokens â†’ ~800,000 caractÃ¨res de contexte (50% rÃ©servÃ©)
- GPT-4o/Turbo: 128,000 tokens â†’ ~256,000 caractÃ¨res de contexte
- GPT-4: 8,192 tokens â†’ ~16,384 caractÃ¨res de contexte
- GPT-3.5 Turbo: 16,385 tokens â†’ ~32,770 caractÃ¨res de contexte
```

**Avant**: Limite fixe de 8,000 caractÃ¨res pour tous les modÃ¨les
**AprÃ¨s**: Limite adaptative exploitant 50% de la capacitÃ© du modÃ¨le

**Impact**: Les modÃ¨les modernes peuvent dÃ©sormais traiter 30-100x plus de contexte !

---

### 2. **Augmentation du nombre de chunks rÃ©cupÃ©rÃ©s**
**Fichier**: `backend/src/controllers/chatController.ts`

```typescript
// Recherche de similaritÃ© Ã©tendue
await getVectorStore().similaritySearch(message, userId, 10)
```

**Avant**: 3 chunks maximum
**AprÃ¨s**: 10 chunks maximum

**Impact**: Plus de contexte pertinent envoyÃ© Ã  l'IA, meilleure couverture du document

---

### 3. **Chunking optimisÃ© avec overlap augmentÃ©**
**Fichier**: `backend/src/services/documentProcessor.ts`

```typescript
chunkSize = 600 mots  (au lieu de 1000)
overlap = 300 mots    (au lieu de 200)
```

**Avant**: Chunks de 1000 mots avec 200 mots de chevauchement (20%)
**AprÃ¨s**: Chunks de 600 mots avec 300 mots de chevauchement (50%)

**Impact**: 
- Meilleure granularitÃ© : un titre/section n'est plus noyÃ© dans un gros chunk
- Meilleure prÃ©servation du contexte : 50% de chevauchement assure la continuitÃ©

---

### 4. **Chunking sÃ©mantique prÃ©servant la structure**
**Fichier**: `backend/src/services/documentProcessor.ts`

**Nouvelle logique**:
1. DÃ©coupage initial par **paragraphes** (double saut de ligne)
2. Les paragraphes sont regroupÃ©s intelligemment pour respecter la taille de chunk
3. Les paragraphes trop longs sont subdivisÃ©s avec overlap
4. PrÃ©servation de la structure naturelle du document

**Avant**: DÃ©coupage brutal en comptant les mots, peu importe la structure
**AprÃ¨s**: DÃ©coupage respectant les paragraphes et sections

**Impact**: 
- Les titres restent avec leur contenu
- Les sections cohÃ©rentes ne sont pas coupÃ©es au milieu
- Meilleure comprÃ©hension du contexte par l'IA

---

### 5. **Reranking par pertinence des mots-clÃ©s**
**Fichier**: `backend/src/controllers/chatController.ts`

**Nouvelle Ã©tape de traitement**:
```typescript
1. Recherche vectorielle â†’ 10 chunks candidats
2. Extraction des mots-clÃ©s de la question (> 3 lettres)
3. Calcul du score : nombre d'occurrences de chaque mot-clÃ© dans chaque chunk
4. Tri par score dÃ©croissant
5. Envoi des chunks les plus pertinents Ã  l'IA
```

**Avant**: Seule la similaritÃ© vectorielle (embeddings) Ã©tait utilisÃ©e
**AprÃ¨s**: Combinaison de similaritÃ© vectorielle + pertinence par mots-clÃ©s

**Impact**: 
- Les chunks contenant explicitement les termes de la question sont priorisÃ©s
- RÃ©duit les faux positifs de la recherche vectorielle pure
- AmÃ©liore la prÃ©cision des rÃ©ponses

---

## ğŸ“Š Comparaison Avant/AprÃ¨s

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| **Contexte max (GPT-4o)** | 8,000 chars | 256,000 chars | **32x** |
| **Contexte max (GPT-5)** | 8,000 chars | 800,000 chars | **100x** |
| **Chunks rÃ©cupÃ©rÃ©s** | 3 | 10 | **3.3x** |
| **GranularitÃ©** | 1000 mots/chunk | 600 mots/chunk | **+40%** |
| **Overlap** | 20% | 50% | **+150%** |
| **PrÃ©servation structure** | âŒ Non | âœ… Oui | âœ¨ |
| **Reranking** | âŒ Non | âœ… Oui | âœ¨ |

---

## ğŸ”§ Exemple concret

### Votre cas d'usage : PDF avec section "La voie professionnelle"

**ProblÃ¨me initial**:
- Question: "Le document Ã©voque la voie professionnelle, tu peux m'en dire plus ?"
- RÃ©ponse: "Il n'y a aucune information relative Ã  la voie professionnelle"

**Pourquoi ?**
1. Le titre Ã©tait dans un chunk de 1000 mots avec beaucoup d'autres infos
2. Seuls 3 chunks Ã©taient rÃ©cupÃ©rÃ©s, probablement pas celui avec le titre
3. Pas de reranking pour prioriser les chunks avec "voie professionnelle"

**Avec les optimisations**:
1. âœ… Chunks de 600 mots â†’ le titre a plus de chances d'Ãªtre avec son contenu
2. âœ… DÃ©coupage par paragraphes â†’ le titre reste avec sa section
3. âœ… 10 chunks rÃ©cupÃ©rÃ©s â†’ plus de couverture du document
4. âœ… Reranking â†’ les chunks contenant "voie" et "professionnelle" sont priorisÃ©s
5. âœ… Plus de contexte envoyÃ© Ã  l'IA â†’ GPT-4o peut traiter 256k chars au lieu de 8k

**RÃ©sultat attendu**: L'IA trouvera et citera correctement la section sur la voie professionnelle !

---

## ğŸš€ Prochaines amÃ©liorations possibles

Si vous voulez aller encore plus loin :

1. **Extraction de titres**: DÃ©tecter automatiquement les titres (majuscules, mots-clÃ©s, position) et les inclure dans les mÃ©tadonnÃ©es
2. **Embeddings hybrides**: Combiner embeddings sÃ©mantiques + BM25 (recherche TF-IDF)
3. **Chunking rÃ©cursif**: DÃ©couper par chapitres â†’ sections â†’ paragraphes de maniÃ¨re hiÃ©rarchique
4. **Cache de requÃªtes**: MÃ©moriser les questions frÃ©quentes pour accÃ©lÃ©rer les rÃ©ponses
5. **Feedback loop**: Permettre Ã  l'utilisateur d'indiquer si la rÃ©ponse Ã©tait pertinente pour amÃ©liorer le systÃ¨me

---

## ğŸ“ Notes importantes

### Performance
- Les nouveaux paramÃ¨tres peuvent gÃ©nÃ©rer plus de chunks (limite portÃ©e Ã  1000 au lieu de 500)
- Les modÃ¨les avec plus de contexte coÃ»tent plus cher en tokens
- Le reranking ajoute un lÃ©ger dÃ©lai de calcul (~50-100ms)

### Recommandations
- **Pour documents courts** (< 10 pages): Utilisez GPT-4o-mini (Ã©conomique)
- **Pour documents complexes** (> 50 pages): Utilisez GPT-4o ou GPT-5 (plus de contexte)
- **Pour questions spÃ©cifiques**: Le reranking est particuliÃ¨rement efficace
- **Pour questions gÃ©nÃ©rales**: La recherche vectorielle seule suffit

### Limites
- Les trÃ¨s gros documents (> 1 MB) sont toujours tronquÃ©s pour Ã©viter les problÃ¨mes de mÃ©moire
- Le systÃ¨me ne peut pas "comprendre" des images ou graphiques dans les PDF
- Les tableaux complexes peuvent Ãªtre mal formatÃ©s lors de l'extraction

---

## ğŸ§ª Test recommandÃ©

1. **Re-uploadez votre PDF** sur la voie professionnelle (pour que le nouveau chunking soit appliquÃ©)
2. SÃ©lectionnez **GPT-4o** dans le dropdown (pour profiter du grand contexte)
3. Posez la mÃªme question : "Le document Ã©voque la voie professionnelle, tu peux m'en dire plus ?"
4. Comparez la rÃ©ponse !

---

**Date de derniÃ¨re mise Ã  jour**: 23 octobre 2025
**Version du systÃ¨me RAG**: 2.0
